
## Image Captioning using Transformer and CNN

• Developed an image captioning model that converts visual information from images into textual descriptions.
• Utilized the Flicker8k dataset containing 8,000 images with 5 captions each for training and evaluation.
• Preprocessed image data through resizing and normalization techniques.
• Implemented text preprocessing techniques including caption mapping and tokenization for natural language
• processing.
• Leveraged a pre-trained CNN model (EfficientNet) to extract informative feature vectors from images.
• Incorporated a Transformer encoder to generate a rich encoded representation of the extracted image features.
• Employed a Transformer decoder to translate the encoded image representation into human-readable captions
